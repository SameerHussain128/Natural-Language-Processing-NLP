{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb653f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk \n",
    "#nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING TEXT BY STEMMING - PorterStemmer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed129091",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'the best and most beautifull thing in the world cannot be seen or even touched,they must be felt with heart'\n",
    "quotes_tokens = nltk.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N GRAMS\n",
    "quotes_ngrams_1 = list(nltk.ngrams(quotes_tokens, 5)) \n",
    "quotes_ngrams_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020362f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem=['give','giving','given','gave','thinking', 'loving', 'final', 'finalized', 'finally']\n",
    "# i am giving these different words to stem, using porter stemmer we get the output\n",
    "\n",
    "# Next we need to make some changes in tokens and that is called as stemming, stemming will gives you root form of an word\n",
    "# also we will see some root form of the word & limitation of the word\n",
    "\n",
    "#porter-stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words+ ':' +pst.stem(words))\n",
    "    \n",
    "#in porterstemmer removes ing and replaces with e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95dc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometime stemming does not work & lets say e.g - fish,fishes & fishing all of them belongs to root word fish, \n",
    "#one hand stemming will cut the end & lemmatization will take into the morphological analysis of the word\n",
    "\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()\n",
    "\n",
    "#Hear we are going to wordnet dictionary & we are going to import the wordnet lematizer\n",
    "#word_lem.lemmatize('corpora') #we get output as corpus \n",
    "\n",
    "#refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words+ ':' +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is other concept called POS (part of speech) which deals with subject, noun, pronoun but before of this lets go with other concept called STOPWORDS\n",
    "# STOPWORDS = i, is, as,at, on, about & nltk has their own list of stopewords \n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe389f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a966cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to compile from re module to create string that matched any digits or special character \n",
    "import re\n",
    "punctuation = re.compile(r'[-.?!,:;()|0-9]') \n",
    "\n",
    "#now i am going to create to empty list and append the word without any punctuation & naming this as a post punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "# we will see how to work in POS using NLTK library\n",
    "\n",
    "sent = 'kathy is a natural when it comes to drawing'\n",
    "sent_tokens = word_tokenize(sent)\n",
    "sent_tokens\n",
    "\n",
    "# first we will tokenize usning word_tokenize & then we will use pos_tag on all of the tokens \n",
    "\n",
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))\n",
    "    \n",
    "    \n",
    "# OUTPUT :\n",
    "[('kathy', 'NN')]\n",
    "[('is', 'VBZ')]\n",
    "[('a', 'DT')]\n",
    "[('natural', 'JJ')]\n",
    "[('when', 'WRB')]\n",
    "[('it', 'PRP')]\n",
    "[('comes', 'VBZ')]\n",
    "[('to', 'TO')]\n",
    "[('drawing', 'VBG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "NE_sent = 'The US president stays in the WHITEHOUSE '\n",
    "# chunking means grouping of words into chunks & lets understand the example of chunking \n",
    "# chunking will help to easy process the data\n",
    "NE_tokens = word_tokenize(NE_sent)\n",
    "\n",
    "#after tokenize need to add the pos tags\n",
    "NE_tokens\n",
    "\n",
    "NE_tags = nltk.pos_tag(NE_tokens)\n",
    "NE_tags\n",
    "\n",
    "\n",
    "#we are passin the NE_NER into ne_chunks function and lets see the outputs\n",
    "\n",
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=480, height=480, margin=0).generate(text) \n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838585d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "\n",
    "# Tokenization of paragraphs/sentences\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career. india is great compare to other country \"\"\"\n",
    "               \n",
    "# Tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Tokenizing words\n",
    "words = nltk.word_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#using porter stemmer you can always stem the word called from PorterStemmer \n",
    "#poterstemmer librarty will help you to find the root word \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#STOPWORDS – stopwords are present inside the nltk.corpus \n",
    "#why we use stopwords hear becuase if you see below para then we do see some word called of, the, and , our, of , them \n",
    "#this kind of words are repeted again and again & these words are no such a meaningfull information & there is no meaning when we assign to the computer or machine\n",
    "#stopwords help us to remove this kind of words in the paragraph for better text preprocessing\n",
    "#most of the senario we do apply stopwords\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "                          \n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "stemmer = PorterStemmer()\n",
    "#import stopwords\n",
    "# I want to remove all the stopwords from my senterences \n",
    "# if you check the stopwords.words('english') you get a list of word which is not at all value to the paragraph\n",
    "# you do get stopwords in many language. \n",
    "# after removing the stopwords i am going to stem the words by using portstemmer\n",
    "\n",
    "# using for loop for all of sentences & using word_tokenize will convert all sentences to words\n",
    "# basically i am writhing for word in words and i am taking from unique word from stopword.english\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lEMMATIZATION\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# wordnetlematizer library is the responsible for doing the lemmatization function\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n",
    "               \n",
    "               \n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e43fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "\n",
    "# Cleaning the texts\n",
    "import re # re libray will use for regular expression \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# Create the empty list name as corpus becuase after cleaned the data corpus will store this clean data\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "#    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]   \n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the Bag of Words model \n",
    "\n",
    "# Also we called as document matrix \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c01a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d78af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
